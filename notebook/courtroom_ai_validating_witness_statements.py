# -*- coding: utf-8 -*-
"""Courtroom AI- Validating Witness Statements.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XXvekEjcbdQHuha9vc0fENHfdGwVHcJv
"""
import sys
import spacy
import nltk
import pandas as pd
import numpy as np
import re
import json
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.util import ngrams
from nltk.wsd import lesk
from nltk.corpus import wordnet as wn
from nltk.corpus import brown
from nltk.probability import FreqDist, ConditionalFreqDist
from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures
from collections import Counter
import matplotlib.pyplot as plt
from datasets import load_dataset
from tqdm import tqdm

# Download required NLTK resources
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('brown')
nltk.download('omw-1.4')

# Load spaCy model
try:
    nlp = spacy.load('en_core_web_sm')
except IOError:
    print("Downloading spaCy model...")
    import subprocess
    subprocess.run([sys.executable, "-m", "spacy", "download", "en_core_web_sm"])
    nlp = spacy.load('en_core_web_sm')
    
# Load dataset
print("Loading the LAR-ECHR dataset...")
ds = load_dataset("AUEB-NLP/lar-echr")
print("Dataset loaded successfully.")

# Display information about the dataset
print("Dataset structure:")
print(ds)

# Update the extract_sample_texts function to better handle the dataset structure
def extract_sample_texts(dataset, max_samples=1000):
    """Extract sample texts from the dataset using facts and complete_facts fields"""
    texts = []

    # Process each split in the dataset
    for split in ['train', 'validation', 'test']:
        if split in dataset:
            # Determine number of samples to use (all or max_samples)
            n_samples = min(len(dataset[split]), max_samples // len(dataset.keys()))

            print(f"Extracting {n_samples} samples from {split} split")

            # Extract texts from this split
            for i in range(n_samples):
                item = dataset[split][i]

                # Try complete_facts first as it's likely more comprehensive
                if 'complete_facts' in item and item['complete_facts'] and len(item['complete_facts']) > 0:
                    texts.append(item['complete_facts'])
                # Fall back to facts if complete_facts is not available
                elif 'facts' in item and item['facts'] and len(item['facts']) > 0:
                    texts.append(item['facts'])

    print(f"Total extracted texts: {len(texts)}")
    return texts

# Function to convert dataset to pandas DataFrame for easier exploration
def dataset_to_dataframe(dataset, split='train'):
    """Convert a dataset split to pandas DataFrame"""
    if split not in dataset:
        return None

    # Convert to pandas DataFrame
    df = pd.DataFrame(dataset[split])
    return df

# Create a legal text corpus for bigram analysis
sample_texts = extract_sample_texts(ds)
print(f"Extracted {len(sample_texts)} sample texts for bigram analysis")

# Class for handling morphological analysis
class MorphologicalAnalyzer:
    def __init__(self):
        self.nlp = nlp

    def analyze_text(self, text):
        """Analyze morphology of text"""
        doc = self.nlp(text)

        # Extract tense information
        tense_info = self._identify_tense(doc)

        # Extract root forms
        root_forms = self._extract_root_forms(doc)

        return {
            "Tense": tense_info,
            "Root Verbs": root_forms
        }

    def _identify_tense(self, doc):
        """Identify tense of verbs in the document"""
        tenses = []

        for token in doc:
            if token.pos_ == "VERB" or token.pos_ == "AUX":
                tense = self._determine_tense(token)
                if tense and tense not in tenses:
                    tenses.append(tense)

        return tenses

    def _determine_tense(self, token):
        """Determine tense of a verb token"""
        # Simple rules for tense determination
        if token.tag_ in ["VB", "VBP"]:
            return "Present Simple"
        elif token.tag_ == "VBZ":
            return "Present Simple (3rd person)"
        elif token.tag_ == "VBD":
            return "Past Simple"
        elif token.tag_ == "VBG" and any(aux.lemma_ == "be" for aux in token.head.children):
            return "Present Continuous"
        elif token.tag_ == "VBN":
            # Check for perfect tenses
            aux_verbs = [child for child in token.head.children if child.dep_ == "aux"]
            if any(aux.lemma_ == "have" for aux in aux_verbs):
                return "Present Perfect"
            elif any(aux.lemma_ == "be" for aux in aux_verbs):
                return "Passive Voice"

        return None

    def _extract_root_forms(self, doc):
        """Extract root forms of verbs"""
        roots = []

        for token in doc:
            if token.pos_ == "VERB":
                roots.append(token.lemma_)

        return list(set(roots))

# Class for syntactic parsing
class SyntacticParser:
    def __init__(self):
        self.nlp = nlp

    def analyze_syntax(self, text):
        """Analyze syntax of text"""
        doc = self.nlp(text)

        # Check for fragments
        fragments = self._detect_fragments(doc)

        # Check for contradictions
        contradictions = self._detect_contradictions(doc)

        # Check for subject-verb agreement
        sv_agreement = self._check_subject_verb_agreement(doc)

        result = {
            "Fragments": fragments,
            "Contradictions": contradictions,
            "Subject-Verb Agreement": sv_agreement
        }

        if fragments or contradictions or not sv_agreement:
            result["Syntax Check"] = "Issues Detected"
        else:
            result["Syntax Check"] = "No Issues Detected"

        return result

    def _detect_fragments(self, doc):
        """Detect sentence fragments"""
        fragments = []

        for sent in doc.sents:
            # Check if the sentence has a root verb
            has_root_verb = False
            for token in sent:
                if token.dep_ == "ROOT" and token.pos_ == "VERB":
                    has_root_verb = True
                    break

            if not has_root_verb:
                fragments.append(sent.text)

        return fragments

    def _detect_contradictions(self, doc):
        """Detect potential contradictions in the text"""
        contradictions = []

        # Basic contradiction patterns
        negation_terms = ["not", "didn't", "don't", "doesn't", "never", "no"]
        knowledge_terms = ["know", "sure", "certain", "definitely"]
        seeing_terms = ["see", "saw", "witness", "observe", "observed"]

        text_lower = doc.text.lower()

        # Check for common contradiction patterns
        for negation in negation_terms:
            for seeing in seeing_terms:
                if f"{negation} {seeing}" in text_lower:
                    for knowledge in knowledge_terms:
                        if knowledge in text_lower:
                            contradictions.append(f"Contradiction: '{negation} {seeing}' vs '{knowledge}'")

        return contradictions

    def _check_subject_verb_agreement(self, doc):
        """Check subject-verb agreement"""
        for token in doc:
            if token.dep_ == "nsubj" and token.head.pos_ == "VERB":
                # Simple check for third person singular
                if token.text.lower() in ["he", "she", "it"] and token.head.tag_ not in ["VBZ", "VBD"]:
                    return False

        return True

# Class for bigram analysis
class BigramAnalyzer:
    def __init__(self):
        self.bigram_fd = None
        self.total_bigrams = 0

    def train_bigram_model(self, texts):
        """Train bigram model on legal texts"""
        all_words = []
        for text in texts:
            words = word_tokenize(text.lower())
            all_words.extend(words)

        # Create bigram frequency distribution
        bigrams = list(ngrams(all_words, 2))
        self.bigram_fd = FreqDist(bigrams)
        self.total_bigrams = len(bigrams)

        print(f"Trained bigram model on {self.total_bigrams} bigrams")

    def calculate_bigram_probability(self, text):
        """Calculate bigram probability for the text"""
        if not self.bigram_fd:
            return {"Bigram Probability": "Model not trained"}

        words = word_tokenize(text.lower())
        if len(words) < 2:
            return {"Bigram Probability": "Text too short"}

        # Calculate bigram probabilities
        text_bigrams = list(ngrams(words, 2))
        probabilities = []

        for bigram in text_bigrams:
            count = self.bigram_fd[bigram]
            # Smooth probabilities
            prob = (count + 1) / (self.total_bigrams + 1)
            probabilities.append(prob)

        avg_prob = sum(probabilities) / len(probabilities)

        # Assess the probability
        if avg_prob < 0.0001:
            assessment = "Very Low"
        elif avg_prob < 0.001:
            assessment = "Low"
        elif avg_prob < 0.01:
            assessment = "Moderate"
        else:
            assessment = "High"

        return {
            "Bigram Probability": f"{assessment} ({avg_prob:.6f})",
            "Raw Probability": avg_prob
        }

# Class for POS tagging
class POSTagger:
    def __init__(self):
        self.nlp = nlp

    def analyze_pos(self, text):
        """Analyze part of speech in the text"""
        doc = self.nlp(text)

        # Get POS tags
        pos_tags = {token.text: token.tag_ for token in doc}

        # Check for passive voice
        passive_constructions = self._detect_passive_voice(doc)

        # Detect questions
        questions = self._detect_questions(doc)

        # Detect modal verbs
        modals = self._detect_modals(doc)

        return {
            "PoS Tags": pos_tags,
            "Passive Voice": passive_constructions,
            "Questions": questions,
            "Modal Verbs": modals
        }

    def _detect_passive_voice(self, doc):
        """Detect passive voice constructions"""
        passive_constructions = []

        for token in doc:
            if token.dep_ == "nsubjpass":
                passive_constructions.append(token.sent.text)

        return passive_constructions

    def _detect_questions(self, doc):
        """Detect questions in the text"""
        questions = []

        for sent in doc.sents:
            if sent.text.strip().endswith("?"):
                questions.append(sent.text)
            elif sent.root.tag_ in ["VBZ", "VBP", "VBD"] and sent[0].text.lower() in ["do", "does", "did", "is", "are", "was", "were"]:
                questions.append(sent.text)

        return questions

    def _detect_modals(self, doc):
        """Detect modal verbs indicating uncertainty"""
        uncertainty_modals = ["might", "could", "may", "perhaps", "possibly", "probably", "think", "believe", "assume"]
        modals_found = []

        for token in doc:
            if token.lemma_.lower() in uncertainty_modals:
                modals_found.append(token.text)

        return modals_found

# Class for Word Sense Disambiguation
class WordSenseDisambiguator:
    def __init__(self):
        self.legal_terms = {
            "case": ["legal proceeding", "container", "instance"],
            "charge": ["accuse formally", "request payment", "electrical property"],
            "right": ["entitlement", "direction", "correct"],
            "present": ["current", "gift", "introduce"],
            "court": ["legal institution", "enclosed area", "royal residence"],
            "appeal": ["request review", "attractiveness", "request for help"],
            "sentence": ["punishment", "grammatical unit"],
            "hearing": ["legal proceeding", "auditory sensation"],
            "defendant": ["accused person"],
            "record": ["documentation", "phonograph", "best performance"],
            "motion": ["formal request", "movement", "gesture"],
            "party": ["celebration", "political group", "person in legal case"],
            "bar": ["legal profession", "counter serving drinks", "obstacle"],
            "bench": ["seat", "judge's seat", "reserve players"],
            "brief": ["concise", "legal document", "short duration"]
        }

    def disambiguate(self, text):
        """Disambiguate word senses in text"""
        tokens = word_tokenize(text.lower())
        results = {}

        for word in tokens:
            if word in self.legal_terms:
                # Use lesk algorithm for disambiguation
                best_sense = lesk(tokens, word)

                if best_sense:
                    definition = best_sense.definition()
                    results[word] = definition
                else:
                    # Fallback to listing possible meanings
                    results[word] = self.legal_terms[word]

        return {"WSD": results}

# Main class for witness statement validation
class WitnessStatementValidator:
    def __init__(self):
        self.morphological_analyzer = MorphologicalAnalyzer()
        self.syntactic_parser = SyntacticParser()
        self.bigram_analyzer = BigramAnalyzer()
        self.pos_tagger = POSTagger()
        self.wsd = WordSenseDisambiguator()

        # Model is not trained by default
        self.model_trained = False

    def train_model(self, texts):
        """Train the model on legal texts"""
        self.bigram_analyzer.train_bigram_model(texts)
        self.model_trained = True

    def validate_statement(self, statement):
        """Validate a witness statement"""
        # Check if model is trained
        if not self.model_trained:
            print("Warning: Bigram model not trained. Results may be less accurate.")

        # Run all analyses
        morphology_results = self.morphological_analyzer.analyze_text(statement)
        syntax_results = self.syntactic_parser.analyze_syntax(statement)
        bigram_results = self.bigram_analyzer.calculate_bigram_probability(statement)
        pos_results = self.pos_tagger.analyze_pos(statement)
        wsd_results = self.wsd.disambiguate(statement)

        # Combine results
        results = {
            "Morphology": morphology_results,
            "Syntax": syntax_results,
            **bigram_results,
            "PoS Analysis": pos_results,
            **wsd_results
        }

        # Determine overall verdict
        verdict, reason = self._determine_verdict(results)
        results["Verdict"] = verdict
        results["Reason"] = reason

        return results

    def _determine_verdict(self, results):
        """Determine the verdict based on the analysis results"""
        issues = []

        # Check for contradictions
        if results["Syntax"]["Contradictions"]:
            issues.append("Contradictory statements detected")

        # Check for fragments
        if results["Syntax"]["Fragments"]:
            issues.append("Sentence fragments detected")

        # Check bigram probability
        if "Raw Probability" in results and results["Raw Probability"] < 0.001:
            issues.append("Unusually low language probability")

        # Check for passive voice (especially if numerous)
        if len(results["PoS Analysis"]["Passive Voice"]) > 1:
            issues.append("Heavy use of passive voice")

        # Check for modal verbs indicating uncertainty
        if len(results["PoS Analysis"]["Modal Verbs"]) > 2:
            issues.append("High uncertainty in language")

        # Determine verdict
        if not issues:
            return "Valid Statement", "No significant issues detected"
        elif len(issues) == 1:
            return "Potentially Suspicious", issues[0]
        else:
            return "Suspicious Statement", "; ".join(issues)

# Train and validate the model
def main():
    # Create validator
    validator = WitnessStatementValidator()

    # Train model
    print("Training the model...")
    validator.train_model(sample_texts)

    # Test with sample statements
    sample_statements = [
        "I didn't see anything that night, but I know he was there.",
        "The defendant was observed entering the building at approximately 10:30 PM.",
        "I think I might have heard something, but I'm not sure.",
        "The victim was walking after the shooting.",
        "I saw the accused with my own eyes. He was holding the weapon."
    ]

    for i, statement in enumerate(sample_statements):
        print(f"\n\nAnalyzing statement {i+1}: \"{statement}\"")
        results = validator.validate_statement(statement)

        # Print results in a readable format
        #print(json.dumps(results, indent=2))

    # Interactive mode
    print("\n\nInteractive Mode: Enter a witness statement to analyze (or 'quit' to exit)")
    while True:
        user_input = input("\nEnter statement: ")
        if user_input.lower() == 'quit':
            break

        results = validator.validate_statement(user_input)
        print(json.dumps(results, indent=2))

if __name__ == "__main__":
    main()